{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commmonly used activation functions \n",
    "* ReLU\n",
    "* Sigmoid\n",
    "* Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid (Logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>&#x03C3;<!-- σ --></mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>x</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mfrac>\n",
    "    <mn>1/</mn>\n",
    "    <mrow>\n",
    "      <mn>(1</mn>\n",
    "      <mo>+</mo>\n",
    "      <msup>\n",
    "        <mi>e</mi>\n",
    "        <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "          <mo>&#x2212;<!-- − --></mo>\n",
    "          <mi>x)</mi>\n",
    "        </mrow>\n",
    "      </msup>\n",
    "    </mrow>\n",
    "  </mfrac>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons:\n",
    "* Activation saturates at 0 or 1 with gradients ≈ 0\n",
    "    * No signal to update weights → cannot learn\n",
    "    * Solution: Have to carefully initialize weights to prevent this\n",
    "* Outputs not centered around 0\n",
    "    * If output always positive → gradients always positive or negative → bad for gradient updates\n",
    "    \n",
    "This causes vanishing gradients and poor learning for deep networks. This can occur when the weights of our networks are initialized poorly – with too-large negative and positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tanh(x)=2σ(2x)−1\n",
    " \n",
    "    A scaled sigmoid function\n",
    "* Input number → [-1, 1]\n",
    "* Cons:\n",
    "    * Activation saturates at 0 or 1 with gradients ≈ 0\n",
    "        * No signal to update weights → cannot learn\n",
    "        * Solution: Have to carefully initialize weights to prevent this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ReLUs¶\n",
    "* f(x)=max(0,x)\n",
    "* Pros:\n",
    "    * Accelerates convergence → train faster\n",
    "    * Less computationally expensive operation compared to Sigmoid/Tanh exponentials\n",
    "* Cons:\n",
    "    *Many ReLU units \"die\" → gradients = 0 forever\n",
    "        * Solution: careful learning rate choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:28, 344804.52it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Using downloaded and verified file: ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "train_dataset = datasets.MNIST(root = './data',\n",
    "                                   train = True,\n",
    "                                   transform = transforms.ToTensor(),download = True)\n",
    "\n",
    "\n",
    "test_dataset = datasets.MNIST(root = './data',\n",
    "                                   train = False,\n",
    "                                   transform = transforms.ToTensor(),download = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "n_iters = 3000\n",
    "num_epochs = int(n_iters/(len(train_dataset)/batch_size) ) + 1\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                         shuffle = True, \n",
    "                         batch_size = batch_size)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(train_dataset,\n",
    "                         shuffle = True, \n",
    "                         batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden = 100\n",
    "output_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforwardneuralnetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden, output_dim):\n",
    "        # as we have already told in the linear regression notebook, nn.Module is the parent class here \n",
    "        # and Feedforwardneuralnetwork is the child class that inherit from the parent class.\n",
    "        \n",
    "        \n",
    "        # Then obvious question arise is that, we never calls the forward function although we define it\n",
    "        # its because,there is a inbuilt _call_ function that calls the forward function from the parent class\n",
    "        \n",
    "        super(Feedforwardneuralnetwork, self).__init__()\n",
    "        # first: linear function(first input layer) with relu activation\n",
    "        self.fc1 = nn.Linear(input_dim, hidden)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        # second: Linear layer\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # linear fucntion layer\n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # output layer: linear\n",
    "        self.fc4 = nn.Linear(hidden, output_dim)\n",
    "\n",
    "    def forward(self, x,):\n",
    "        # input layer\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # first hidden layer\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        \n",
    "        # second layer\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        # output layer\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "        \n",
    "model = Feedforwardneuralnetwork(input_dim, hidden, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer: torch.Size([100, 784])\n",
      "second layer: torch.Size([100])\n",
      "third layer: torch.Size([100, 100])\n",
      "fourth layer: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# checking initial weights shape \n",
    "\n",
    "print(\"first layer:\", list(model.parameters())[0].shape)\n",
    "print(\"second layer:\", list(model.parameters())[1].shape)\n",
    "print(\"third layer:\", list(model.parameters())[2].shape)\n",
    "print(\"fourth layer:\", list(model.parameters())[3].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### instantiating the loss fucntion and the optimizer for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate )\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.053387608379125595. Accuracy: 97.69491577148438\n",
      "Iteration: 1000. Loss: 0.061938781291246414. Accuracy: 98.20050811767578\n",
      "Iteration: 1500. Loss: 0.10773777216672897. Accuracy: 98.06912231445312\n",
      "Iteration: 2000. Loss: 0.0716380923986435. Accuracy: 98.41504669189453\n",
      "Iteration: 2500. Loss: 0.02929224818944931. Accuracy: 98.66285705566406\n",
      "Iteration: 3000. Loss: 0.019005168229341507. Accuracy: 98.95722198486328\n"
     ]
    }
   ],
   "source": [
    "iter = 0;\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images, labels) in enumerate(train_loader):\n",
    "        # reshaping the image\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        \n",
    "        # clearing the optimizer gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to get the output\n",
    "        output = model(images)\n",
    "        \n",
    "        # calculate loss: softmax => cross entropy loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # getting gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating the weights with \n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        # validating the current model\n",
    "        if iter%500 == 0:\n",
    "            correct = 0\n",
    "            total = len(labels)\n",
    "            \n",
    "            \n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "                \n",
    "                outputs = model.forward(images)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "                \n",
    "            accuracy = 100*(correct/total)\n",
    "            \n",
    "            \n",
    "#           Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)",
   "language": "python",
   "name": "python37764bitmyenvconda823dfb8e79a54b2babc4116ecd4d023d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
